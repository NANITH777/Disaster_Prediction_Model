{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'UserName', 'Timestamp', 'Verified', 'Tweets', 'Comments',\n",
       "       'Retweets', 'Likes', 'Impressions', 'Tags', 'Tweet Link', 'Tweet ID',\n",
       "       'Disaster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Disaster.csv\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Tweets'].tolist()\n",
    "labels = data['Disaster'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disaster\n",
       "Drought       770\n",
       "Wildfire      540\n",
       "Earthquake    500\n",
       "Floods        436\n",
       "Hurricanes    178\n",
       "Tornadoes     135\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Disaster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary for disaster types\n",
    "disaster_mapping = {\n",
    "    'Drought': 0,\n",
    "    'Wildfire': 1,\n",
    "    'Earthquake': 2,\n",
    "    'Floods': 3,\n",
    "    'Hurricanes': 4,\n",
    "    'Tornadoes': 5\n",
    "}\n",
    "\n",
    "# Apply the mapping to the Disaster column\n",
    "data['Disaster'] = data['Disaster'].map(disaster_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disaster\n",
       "0    770\n",
       "1    540\n",
       "2    500\n",
       "3    436\n",
       "4    178\n",
       "5    135\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Disaster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation: Select 1000 random samples from the dataset\n",
    "data = data.sample(1000, random_state=42 )\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['Tweets'], data['Disaster'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "class DisasterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DisasterDataset(train_encodings, train_labels.tolist())\n",
    "test_dataset = DisasterDataset(test_encodings, test_labels.tolist())\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.8611637353897095\n",
      "Epoch: 0, Loss: 1.6825507879257202\n",
      "Epoch: 0, Loss: 1.6659104824066162\n",
      "Epoch: 0, Loss: 1.7324029207229614\n",
      "Epoch: 0, Loss: 1.352903962135315\n",
      "Epoch: 0, Loss: 1.8923814296722412\n",
      "Epoch: 0, Loss: 1.5810589790344238\n",
      "Epoch: 0, Loss: 1.4255423545837402\n",
      "Epoch: 0, Loss: 1.8504620790481567\n",
      "Epoch: 0, Loss: 1.4758139848709106\n",
      "Epoch: 0, Loss: 1.4438576698303223\n",
      "Epoch: 0, Loss: 1.1957381963729858\n",
      "Epoch: 0, Loss: 1.3832604885101318\n",
      "Epoch: 0, Loss: 1.1025046110153198\n",
      "Epoch: 0, Loss: 1.2231907844543457\n",
      "Epoch: 0, Loss: 1.3225617408752441\n",
      "Epoch: 0, Loss: 1.1537021398544312\n",
      "Epoch: 0, Loss: 1.175886631011963\n",
      "Epoch: 0, Loss: 1.3046786785125732\n",
      "Epoch: 0, Loss: 1.0497820377349854\n",
      "Epoch: 0, Loss: 0.7837339639663696\n",
      "Epoch: 0, Loss: 0.7796065211296082\n",
      "Epoch: 0, Loss: 0.765639066696167\n",
      "Epoch: 0, Loss: 0.7415425181388855\n",
      "Epoch: 0, Loss: 0.7704342007637024\n",
      "Epoch: 0, Loss: 0.8213167190551758\n",
      "Epoch: 0, Loss: 0.7990801334381104\n",
      "Epoch: 0, Loss: 0.694758951663971\n",
      "Epoch: 0, Loss: 0.4947943091392517\n",
      "Epoch: 0, Loss: 0.43236884474754333\n",
      "Epoch: 0, Loss: 0.5811299085617065\n",
      "Epoch: 0, Loss: 0.5254688858985901\n",
      "Epoch: 0, Loss: 0.49695783853530884\n",
      "Epoch: 0, Loss: 0.3348899781703949\n",
      "Epoch: 0, Loss: 0.28918910026550293\n",
      "Epoch: 0, Loss: 0.30760082602500916\n",
      "Epoch: 0, Loss: 0.4774295389652252\n",
      "Epoch: 0, Loss: 0.31454333662986755\n",
      "Epoch: 0, Loss: 0.19970962405204773\n",
      "Epoch: 0, Loss: 0.2545894384384155\n",
      "Epoch: 0, Loss: 0.4634193181991577\n",
      "Epoch: 0, Loss: 0.1718810796737671\n",
      "Epoch: 0, Loss: 0.17284652590751648\n",
      "Epoch: 0, Loss: 0.30950868129730225\n",
      "Epoch: 0, Loss: 0.1969231516122818\n",
      "Epoch: 0, Loss: 0.14958623051643372\n",
      "Epoch: 0, Loss: 0.13299526274204254\n",
      "Epoch: 0, Loss: 0.23930895328521729\n",
      "Epoch: 0, Loss: 0.14365355670452118\n",
      "Epoch: 0, Loss: 0.38495832681655884\n",
      "Epoch: 1, Loss: 0.4338337182998657\n",
      "Epoch: 1, Loss: 0.08403612673282623\n",
      "Epoch: 1, Loss: 0.32041049003601074\n",
      "Epoch: 1, Loss: 0.36287549138069153\n",
      "Epoch: 1, Loss: 0.30097007751464844\n",
      "Epoch: 1, Loss: 0.7309602499008179\n",
      "Epoch: 1, Loss: 0.3515326678752899\n",
      "Epoch: 1, Loss: 0.14572344720363617\n",
      "Epoch: 1, Loss: 0.3796079158782959\n",
      "Epoch: 1, Loss: 0.21710453927516937\n",
      "Epoch: 1, Loss: 0.3110474944114685\n",
      "Epoch: 1, Loss: 0.30291256308555603\n",
      "Epoch: 1, Loss: 0.22709093987941742\n",
      "Epoch: 1, Loss: 0.32033222913742065\n",
      "Epoch: 1, Loss: 0.2541559338569641\n",
      "Epoch: 1, Loss: 0.162406787276268\n",
      "Epoch: 1, Loss: 0.09176678955554962\n",
      "Epoch: 1, Loss: 0.08295689523220062\n",
      "Epoch: 1, Loss: 0.21127302944660187\n",
      "Epoch: 1, Loss: 0.08400928974151611\n",
      "Epoch: 1, Loss: 0.2498009204864502\n",
      "Epoch: 1, Loss: 0.09156864881515503\n",
      "Epoch: 1, Loss: 0.07691354304552078\n",
      "Epoch: 1, Loss: 0.08796602487564087\n",
      "Epoch: 1, Loss: 0.07572624087333679\n",
      "Epoch: 1, Loss: 0.06980718672275543\n",
      "Epoch: 1, Loss: 0.27188390493392944\n",
      "Epoch: 1, Loss: 0.27345576882362366\n",
      "Epoch: 1, Loss: 0.2964702844619751\n",
      "Epoch: 1, Loss: 0.0710546001791954\n",
      "Epoch: 1, Loss: 0.047377537935972214\n",
      "Epoch: 1, Loss: 0.6281578540802002\n",
      "Epoch: 1, Loss: 0.43125855922698975\n",
      "Epoch: 1, Loss: 0.06126723811030388\n",
      "Epoch: 1, Loss: 0.05678858235478401\n",
      "Epoch: 1, Loss: 0.0677214190363884\n",
      "Epoch: 1, Loss: 0.13304224610328674\n",
      "Epoch: 1, Loss: 0.2280779778957367\n",
      "Epoch: 1, Loss: 0.23058123886585236\n",
      "Epoch: 1, Loss: 0.10320407152175903\n",
      "Epoch: 1, Loss: 0.13956283032894135\n",
      "Epoch: 1, Loss: 0.2956332266330719\n",
      "Epoch: 1, Loss: 0.20673231780529022\n",
      "Epoch: 1, Loss: 0.06939471513032913\n",
      "Epoch: 1, Loss: 0.048503439873456955\n",
      "Epoch: 1, Loss: 0.07625211775302887\n",
      "Epoch: 1, Loss: 0.06622638553380966\n",
      "Epoch: 1, Loss: 0.32634812593460083\n",
      "Epoch: 1, Loss: 0.09011136740446091\n",
      "Epoch: 1, Loss: 0.04459194466471672\n",
      "Epoch: 2, Loss: 0.285357266664505\n",
      "Epoch: 2, Loss: 0.43314987421035767\n",
      "Epoch: 2, Loss: 0.0732070803642273\n",
      "Epoch: 2, Loss: 0.21433597803115845\n",
      "Epoch: 2, Loss: 0.06838791072368622\n",
      "Epoch: 2, Loss: 0.1277005821466446\n",
      "Epoch: 2, Loss: 0.18176521360874176\n",
      "Epoch: 2, Loss: 0.3053074777126312\n",
      "Epoch: 2, Loss: 0.042425498366355896\n",
      "Epoch: 2, Loss: 0.03324171155691147\n",
      "Epoch: 2, Loss: 0.4021170735359192\n",
      "Epoch: 2, Loss: 0.04393991082906723\n",
      "Epoch: 2, Loss: 0.04273185133934021\n",
      "Epoch: 2, Loss: 0.05649305135011673\n",
      "Epoch: 2, Loss: 0.384270578622818\n",
      "Epoch: 2, Loss: 0.06583468616008759\n",
      "Epoch: 2, Loss: 0.04463057965040207\n",
      "Epoch: 2, Loss: 0.03734664246439934\n",
      "Epoch: 2, Loss: 0.03974660113453865\n",
      "Epoch: 2, Loss: 0.038049932569265366\n",
      "Epoch: 2, Loss: 0.47122228145599365\n",
      "Epoch: 2, Loss: 0.032459743320941925\n",
      "Epoch: 2, Loss: 0.031259700655937195\n",
      "Epoch: 2, Loss: 0.029367273673415184\n",
      "Epoch: 2, Loss: 0.023774458095431328\n",
      "Epoch: 2, Loss: 0.5515971779823303\n",
      "Epoch: 2, Loss: 0.041557375341653824\n",
      "Epoch: 2, Loss: 0.14367583394050598\n",
      "Epoch: 2, Loss: 0.07591122388839722\n",
      "Epoch: 2, Loss: 0.25044891238212585\n",
      "Epoch: 2, Loss: 0.031048482283949852\n",
      "Epoch: 2, Loss: 0.03432679921388626\n",
      "Epoch: 2, Loss: 0.048364099115133286\n",
      "Epoch: 2, Loss: 0.10311073809862137\n",
      "Epoch: 2, Loss: 0.06552150845527649\n",
      "Epoch: 2, Loss: 0.08665545284748077\n",
      "Epoch: 2, Loss: 0.2476215362548828\n",
      "Epoch: 2, Loss: 0.38276204466819763\n",
      "Epoch: 2, Loss: 0.02983052469789982\n",
      "Epoch: 2, Loss: 0.027346402406692505\n",
      "Epoch: 2, Loss: 0.04109064117074013\n",
      "Epoch: 2, Loss: 0.05284860357642174\n",
      "Epoch: 2, Loss: 0.030207037925720215\n",
      "Epoch: 2, Loss: 0.02867089956998825\n",
      "Epoch: 2, Loss: 0.025895025581121445\n",
      "Epoch: 2, Loss: 0.16284126043319702\n",
      "Epoch: 2, Loss: 0.027651885524392128\n",
      "Epoch: 2, Loss: 0.20794041454792023\n",
      "Epoch: 2, Loss: 0.08704493194818497\n",
      "Epoch: 2, Loss: 0.0313103012740612\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Training for 3 epochs\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
