{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'UserName', 'Timestamp', 'Verified', 'Tweets', 'Comments',\n",
       "       'Retweets', 'Likes', 'Impressions', 'Tags', 'Tweet Link', 'Tweet ID',\n",
       "       'Disaster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Disaster.csv\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Tweets'].tolist()\n",
    "labels = data['Disaster'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disaster\n",
       "Drought       770\n",
       "Wildfire      540\n",
       "Earthquake    500\n",
       "Floods        436\n",
       "Hurricanes    178\n",
       "Tornadoes     135\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Disaster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary for disaster types\n",
    "disaster_mapping = {\n",
    "    'Drought': 0,\n",
    "    'Wildfire': 1,\n",
    "    'Earthquake': 2,\n",
    "    'Floods': 3,\n",
    "    'Hurricanes': 4,\n",
    "    'Tornadoes': 5\n",
    "}\n",
    "\n",
    "# Apply the mapping to the Disaster column\n",
    "data['Disaster'] = data['Disaster'].map(disaster_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disaster\n",
       "0    770\n",
       "1    540\n",
       "2    500\n",
       "3    436\n",
       "4    178\n",
       "5    135\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Disaster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation: Select 1000 random samples from the dataset\n",
    "data = data.sample(1000, random_state=42 )\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['Tweets'], data['Disaster'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "class DisasterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DisasterDataset(train_encodings, train_labels.tolist())\n",
    "test_dataset = DisasterDataset(test_encodings, test_labels.tolist())\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.7568838596343994\n",
      "Epoch: 0, Loss: 1.7284153699874878\n",
      "Epoch: 0, Loss: 1.6571383476257324\n",
      "Epoch: 0, Loss: 1.6723576784133911\n",
      "Epoch: 0, Loss: 1.6334164142608643\n",
      "Epoch: 0, Loss: 1.5641052722930908\n",
      "Epoch: 0, Loss: 1.5680629014968872\n",
      "Epoch: 0, Loss: 1.4901506900787354\n",
      "Epoch: 0, Loss: 1.516200065612793\n",
      "Epoch: 0, Loss: 1.3928720951080322\n",
      "Epoch: 0, Loss: 1.2260676622390747\n",
      "Epoch: 0, Loss: 1.4737558364868164\n",
      "Epoch: 0, Loss: 1.391014575958252\n",
      "Epoch: 0, Loss: 1.257218360900879\n",
      "Epoch: 0, Loss: 1.576425552368164\n",
      "Epoch: 0, Loss: 1.099271297454834\n",
      "Epoch: 0, Loss: 1.1797534227371216\n",
      "Epoch: 0, Loss: 1.0894882678985596\n",
      "Epoch: 0, Loss: 0.9825670123100281\n",
      "Epoch: 0, Loss: 1.081568717956543\n",
      "Epoch: 0, Loss: 0.7816423177719116\n",
      "Epoch: 0, Loss: 0.8939167261123657\n",
      "Epoch: 0, Loss: 0.9672996401786804\n",
      "Epoch: 0, Loss: 0.9492265582084656\n",
      "Epoch: 0, Loss: 0.611672580242157\n",
      "Epoch: 0, Loss: 0.7284276485443115\n",
      "Epoch: 0, Loss: 0.6656630039215088\n",
      "Epoch: 0, Loss: 0.6229923963546753\n",
      "Epoch: 0, Loss: 0.650982141494751\n",
      "Epoch: 0, Loss: 0.45596420764923096\n",
      "Epoch: 0, Loss: 0.5791584253311157\n",
      "Epoch: 0, Loss: 0.3928387463092804\n",
      "Epoch: 0, Loss: 0.5069178938865662\n",
      "Epoch: 0, Loss: 0.40294259786605835\n",
      "Epoch: 0, Loss: 0.45517826080322266\n",
      "Epoch: 0, Loss: 0.23858076333999634\n",
      "Epoch: 0, Loss: 0.25594234466552734\n",
      "Epoch: 0, Loss: 0.30679380893707275\n",
      "Epoch: 0, Loss: 0.3272639214992523\n",
      "Epoch: 0, Loss: 0.18659208714962006\n",
      "Epoch: 0, Loss: 0.4625125527381897\n",
      "Epoch: 0, Loss: 0.4023362398147583\n",
      "Epoch: 0, Loss: 0.25129514932632446\n",
      "Epoch: 0, Loss: 0.22336970269680023\n",
      "Epoch: 0, Loss: 0.10826238244771957\n",
      "Epoch: 0, Loss: 0.6189844608306885\n",
      "Epoch: 0, Loss: 0.2809571623802185\n",
      "Epoch: 0, Loss: 0.1326807290315628\n",
      "Epoch: 0, Loss: 0.2516266703605652\n",
      "Epoch: 0, Loss: 0.3081889748573303\n",
      "Epoch: 1, Loss: 0.12573517858982086\n",
      "Epoch: 1, Loss: 0.12357433140277863\n",
      "Epoch: 1, Loss: 0.30890437960624695\n",
      "Epoch: 1, Loss: 0.11889351904392242\n",
      "Epoch: 1, Loss: 0.11225854605436325\n",
      "Epoch: 1, Loss: 0.08030655980110168\n",
      "Epoch: 1, Loss: 0.09201466292142868\n",
      "Epoch: 1, Loss: 0.24118034541606903\n",
      "Epoch: 1, Loss: 0.31170356273651123\n",
      "Epoch: 1, Loss: 0.22069838643074036\n",
      "Epoch: 1, Loss: 0.17109450697898865\n",
      "Epoch: 1, Loss: 0.10662493109703064\n",
      "Epoch: 1, Loss: 0.24152445793151855\n",
      "Epoch: 1, Loss: 0.07841780036687851\n",
      "Epoch: 1, Loss: 0.30987587571144104\n",
      "Epoch: 1, Loss: 0.24268439412117004\n",
      "Epoch: 1, Loss: 0.27015894651412964\n",
      "Epoch: 1, Loss: 0.047410354018211365\n",
      "Epoch: 1, Loss: 0.23482368886470795\n",
      "Epoch: 1, Loss: 0.06455396860837936\n",
      "Epoch: 1, Loss: 0.059903334826231\n",
      "Epoch: 1, Loss: 0.20098155736923218\n",
      "Epoch: 1, Loss: 0.16747665405273438\n",
      "Epoch: 1, Loss: 0.06309597194194794\n",
      "Epoch: 1, Loss: 0.05681660771369934\n",
      "Epoch: 1, Loss: 0.4050200581550598\n",
      "Epoch: 1, Loss: 0.1761428713798523\n",
      "Epoch: 1, Loss: 0.10842575132846832\n",
      "Epoch: 1, Loss: 0.11160389333963394\n",
      "Epoch: 1, Loss: 0.15819916129112244\n",
      "Epoch: 1, Loss: 0.08541452884674072\n",
      "Epoch: 1, Loss: 0.055295150727033615\n",
      "Epoch: 1, Loss: 0.05721369758248329\n",
      "Epoch: 1, Loss: 0.05155230313539505\n",
      "Epoch: 1, Loss: 0.293956995010376\n",
      "Epoch: 1, Loss: 0.06311751902103424\n",
      "Epoch: 1, Loss: 0.11433448642492294\n",
      "Epoch: 1, Loss: 0.3173869252204895\n",
      "Epoch: 1, Loss: 0.26473763585090637\n",
      "Epoch: 1, Loss: 0.31399112939834595\n",
      "Epoch: 1, Loss: 0.047583457082509995\n",
      "Epoch: 1, Loss: 0.1968124657869339\n",
      "Epoch: 1, Loss: 0.06341750174760818\n",
      "Epoch: 1, Loss: 0.12805770337581635\n",
      "Epoch: 1, Loss: 0.1239030659198761\n",
      "Epoch: 1, Loss: 0.05857768654823303\n",
      "Epoch: 1, Loss: 0.3578222393989563\n",
      "Epoch: 1, Loss: 0.28202641010284424\n",
      "Epoch: 1, Loss: 0.07358979433774948\n",
      "Epoch: 1, Loss: 0.13730530440807343\n",
      "Epoch: 2, Loss: 0.06696140766143799\n",
      "Epoch: 2, Loss: 0.08537355065345764\n",
      "Epoch: 2, Loss: 0.07352244853973389\n",
      "Epoch: 2, Loss: 0.04661513492465019\n",
      "Epoch: 2, Loss: 0.0569833442568779\n",
      "Epoch: 2, Loss: 0.06303242594003677\n",
      "Epoch: 2, Loss: 0.36925846338272095\n",
      "Epoch: 2, Loss: 0.04613158106803894\n",
      "Epoch: 2, Loss: 0.14192944765090942\n",
      "Epoch: 2, Loss: 0.03740239888429642\n",
      "Epoch: 2, Loss: 0.15216250717639923\n",
      "Epoch: 2, Loss: 0.04127952456474304\n",
      "Epoch: 2, Loss: 0.1832989901304245\n",
      "Epoch: 2, Loss: 0.03929244726896286\n",
      "Epoch: 2, Loss: 0.03249654546380043\n",
      "Epoch: 2, Loss: 0.038931477814912796\n",
      "Epoch: 2, Loss: 0.33813032507896423\n",
      "Epoch: 2, Loss: 0.04424802213907242\n",
      "Epoch: 2, Loss: 0.17186379432678223\n",
      "Epoch: 2, Loss: 0.0979381799697876\n",
      "Epoch: 2, Loss: 0.03148693963885307\n",
      "Epoch: 2, Loss: 0.023323895409703255\n",
      "Epoch: 2, Loss: 0.24373961985111237\n",
      "Epoch: 2, Loss: 0.04865854233503342\n",
      "Epoch: 2, Loss: 0.033834632486104965\n",
      "Epoch: 2, Loss: 0.06606383621692657\n",
      "Epoch: 2, Loss: 0.6949843168258667\n",
      "Epoch: 2, Loss: 0.03729723393917084\n",
      "Epoch: 2, Loss: 0.06479716300964355\n",
      "Epoch: 2, Loss: 0.22541086375713348\n",
      "Epoch: 2, Loss: 0.08025924116373062\n",
      "Epoch: 2, Loss: 0.0803476870059967\n",
      "Epoch: 2, Loss: 0.14815805852413177\n",
      "Epoch: 2, Loss: 0.041831959038972855\n",
      "Epoch: 2, Loss: 0.27067187428474426\n",
      "Epoch: 2, Loss: 0.11616020649671555\n",
      "Epoch: 2, Loss: 0.31154581904411316\n",
      "Epoch: 2, Loss: 0.054607026278972626\n",
      "Epoch: 2, Loss: 0.03671140596270561\n",
      "Epoch: 2, Loss: 0.22517850995063782\n",
      "Epoch: 2, Loss: 0.08823288977146149\n",
      "Epoch: 2, Loss: 0.06409148126840591\n",
      "Epoch: 2, Loss: 0.03492263704538345\n",
      "Epoch: 2, Loss: 0.2583670914173126\n",
      "Epoch: 2, Loss: 0.11470890045166016\n",
      "Epoch: 2, Loss: 0.022824635729193687\n",
      "Epoch: 2, Loss: 0.02855396270751953\n",
      "Epoch: 2, Loss: 0.02689043991267681\n",
      "Epoch: 2, Loss: 0.22038710117340088\n",
      "Epoch: 2, Loss: 0.033529132604599\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Training for 3 epochs\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize lists to store true labels and predictions\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "# Evaluate the model\n",
    "for batch in test_dataloader:\n",
    "    inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "    labels = batch['labels']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n",
      "Precision: 0.976047619047619\n",
      "Recall: 0.975\n",
      "F1-Score: 0.9749450454511307\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save = \"disaster_model.pth\"\n",
    "torch.save(model.state_dict(), model_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.txt',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_22280\\13657658.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "model.load_state_dict(torch.load(model_save))\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer/\")\n",
    "\n",
    "# Example prediction\n",
    "new_texts = [\"The smoke from the wildfire is affecting air quality in nearby cities.\"]\n",
    "new_encodings = tokenizer(new_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**new_encodings)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    print(predictions.item()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 263, 1: 164, 2: 144, 3: 138, 4: 49, 5: 42}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_predict = [\n",
    "    \"The prolonged drought is severely affecting agricultural output in the region.\",\n",
    "    \"The earthquake caused extensive damage to buildings and infrastructure in the city.\",\n",
    "    \"Wildfires are raging through the forest, threatening homes and wildlife.\",\n",
    "    \"Heavy rains have caused severe flooding in the downtown area.\",\n",
    "    \"The hurricane made landfall last night, causing widespread power outages.\",\n",
    "    \"A series of tornadoes have torn through the region, causing widespread destruction.\",\n",
    "    \"The hurricane's strong winds and heavy rains have led to significant damage.\",\n",
    "    \"Emergency shelters have been set up to accommodate those displaced by the hurricane.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "encodings = tokenizer(texts_to_predict, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations\n",
    "    outputs = model(**encodings)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get the index of the highest logit for each example\n",
    "\n",
    "# Convert predictions to a list\n",
    "predicted_labels = predictions.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The prolonged drought is severely affecting agricultural output in the region.\n",
      "Predicted Disaster: Drought\n",
      "--------------------------------------------------\n",
      "Text: The earthquake caused extensive damage to buildings and infrastructure in the city.\n",
      "Predicted Disaster: Earthquake\n",
      "--------------------------------------------------\n",
      "Text: Wildfires are raging through the forest, threatening homes and wildlife.\n",
      "Predicted Disaster: Wildfire\n",
      "--------------------------------------------------\n",
      "Text: Heavy rains have caused severe flooding in the downtown area.\n",
      "Predicted Disaster: Floods\n",
      "--------------------------------------------------\n",
      "Text: The hurricane made landfall last night, causing widespread power outages.\n",
      "Predicted Disaster: Hurricanes\n",
      "--------------------------------------------------\n",
      "Text: A series of tornadoes have torn through the region, causing widespread destruction.\n",
      "Predicted Disaster: Tornadoes\n",
      "--------------------------------------------------\n",
      "Text: The hurricane's strong winds and heavy rains have led to significant damage.\n",
      "Predicted Disaster: Hurricanes\n",
      "--------------------------------------------------\n",
      "Text: Emergency shelters have been set up to accommodate those displaced by the hurricane.\n",
      "Predicted Disaster: Hurricanes\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Reverse the disaster mapping\n",
    "reverse_disaster_mapping = {v: k for k, v in disaster_mapping.items()}\n",
    "\n",
    "# Convert numeric labels to disaster types\n",
    "predicted_disasters = [reverse_disaster_mapping[label] for label in predicted_labels]\n",
    "\n",
    "# Print predictions\n",
    "for text, disaster in zip(texts_to_predict, predicted_disasters):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Disaster: {disaster}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
